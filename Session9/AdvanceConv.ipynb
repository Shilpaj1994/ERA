{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advance Convolutions\n"
      ],
      "metadata": {
        "id": "s2CgbyV_ctMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OaTKfnImO_C",
        "outputId": "243ca47c-d848-4bfa-9856-7d700e4debcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Downloading repository on Colab...\n",
            "Cloning into 'ERA'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 190 (delta 64), reused 169 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (190/190), 27.42 MiB | 29.22 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ]
        }
      ],
      "source": [
        "# Import all the required modules\n",
        "import math\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Convolution\n",
        "\n",
        "- Used as a **Feature Extractor**\n",
        "- Most commonly used as Nvidia hardware and software is optimized for 3x3 kernels"
      ],
      "metadata": {
        "id": "Sm7oq29CcxsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassicConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(ClassicConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, bias=False, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(ClassicConv(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR4Gmnphmbwh",
        "outputId": "e95af0ea-4948-4dc8-e064-c91a476e9377"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 32, 32]             270\n",
            "================================================================\n",
            "Total params: 270\n",
            "Trainable params: 270\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.08\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.09\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Pointwise Convolution\n",
        "- Used to **Combine Features**\n",
        "- It is also used to increase or decrease the number of channels in a layer"
      ],
      "metadata": {
        "id": "ZxS-4WVZdDdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(PointConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1: Increase number of channels\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=1, bias=False, padding=0)\n",
        "        )\n",
        "\n",
        "        # Convolutional Block-2: Decrease number of channels\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=3, kernel_size=1, bias=False, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "\n",
        "        x = x.view(-1, 3)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(PointConv(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky4qheTndLD1",
        "outputId": "f9bc44c1-6d8b-48e4-d144-1f31c0772027"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 32, 32]              30\n",
            "            Conv2d-2            [-1, 3, 32, 32]              30\n",
            "================================================================\n",
            "Total params: 60\n",
            "Trainable params: 60\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.10\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.11\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Depthwise Separable Convolution\n",
        "- Used to **reduce number of parameters** in a layer\n",
        "- As a feature extractor, it is 15-20% less efficient compared to the conventional convolution\n",
        "- Instead of having a kernel with same number of channels as of input, each input channels is convolved with separate single channel kernel"
      ],
      "metadata": {
        "id": "7M-m4Zhjc1jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(DepthConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, bias=False, padding=1, groups=3),\n",
        "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=1, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(DepthConv(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mDK-YAYnoP_",
        "outputId": "a25935cb-10fa-4d49-9da1-a3c7f3bc77fe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 3, 32, 32]              27\n",
            "            Conv2d-2           [-1, 10, 32, 32]              30\n",
            "================================================================\n",
            "Total params: 57\n",
            "Trainable params: 57\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.10\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.11\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dilated/Atrous Convolutions\n",
        "- Used to **increase receptive field of the network exponentially**\n",
        "- Used for **dense predictions** - semantic/panoptic segmentations, super-resolution, denoising, generative art, keypoint detection, pose estimation, etc\n",
        "- They help to identify the continuation of the feature and hence they are used after normal convolution layer"
      ],
      "metadata": {
        "id": "lil3H35Kc78D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DilatedConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(DilatedConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3, bias=False, padding=1)\n",
        "        )\n",
        "\n",
        "        # Dilated Convolution Block-2\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=3, bias=False, padding=0, dilation=2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(DilatedConv(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIQYBZdkoB33",
        "outputId": "25b32449-d41e-4783-b574-73e70d6f5300"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 20, 32, 32]             540\n",
            "            Conv2d-2           [-1, 10, 28, 28]           1,800\n",
            "================================================================\n",
            "Total params: 2,340\n",
            "Trainable params: 2,340\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.22\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.24\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Spatially Separable Convolution\n",
        "- Used to **reduce total number of parameters**\n",
        "- It was immensely used in different variants of Xception-Inception Networks as well as in MobileNets\n",
        "- It is obsolete since mobile phone hardware is made powerful enough to handle normal convolutions"
      ],
      "metadata": {
        "id": "RsnLTKAveLKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(SeparableConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1: Increase number of channels\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(3, 1), bias=False, padding=0),\n",
        "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=(1, 3), bias=False, padding=0)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "\n",
        "        x = x.view(-1, 3)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(SeparableConv(), (3, 32, 32))"
      ],
      "metadata": {
        "id": "Pt8d7HPVvBhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ddda00-771f-4a07-bc5e-5c8dd05e3baa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 3, 30, 32]              27\n",
            "            Conv2d-2           [-1, 10, 30, 30]              90\n",
            "================================================================\n",
            "Total params: 117\n",
            "Trainable params: 117\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.09\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Transpose Convolution\n",
        "- Used to **increase channel size after convolution**\n",
        "- Used in all the dense problems"
      ],
      "metadata": {
        "id": "wcHTK6nneRRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransposeConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for demo PyTorch Implementation\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # Initialize the Module class\n",
        "        super(TransposeConv, self).__init__()\n",
        "\n",
        "        # Convolutional Block-1: Increase size of output channel\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=3, out_channels=10, kernel_size=3, bias=False, stride=1, padding=0, output_padding=0)\n",
        "            )\n",
        "\n",
        "        # Convolutional Block-2: Increase size of output channel\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=10, out_channels=10, kernel_size=3, bias=False, stride=2, padding=0, output_padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for model training\n",
        "        :param x: Input layer\n",
        "        :return: Output of the model\n",
        "        \"\"\"\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "summary(TransposeConv(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or74vytyeUNQ",
        "outputId": "77f4fdd8-e010-4ebd-eebb-2a1ecb19fe18"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1           [-1, 10, 34, 34]             270\n",
            "   ConvTranspose2d-2           [-1, 10, 70, 70]             900\n",
            "================================================================\n",
            "Total params: 1,170\n",
            "Trainable params: 1,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.46\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.48\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}