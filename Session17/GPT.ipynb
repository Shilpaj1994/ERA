{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g93xnq7HFSnP"
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqzOoRZYFbcq"
   },
   "source": [
    "## Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKBa9R_QFdxc",
    "outputId": "6663e2d0-03ce-4988-b839-c44f9d4b4c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rp6J7GaFWBU"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N1fPPsGKFXt6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer  # pip install transformers\n",
    "from super_repo.gpt_utils import (\n",
    "    BATCH_SIZE,\n",
    "    BLOCK_SIZE,\n",
    "    DEVICE,\n",
    "    DROPOUT,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EMBED,\n",
    "    NUM_HEAD,\n",
    "    NUM_LAYER,\n",
    "    MAX_ITER,\n",
    "    EVAL_INTER,\n",
    "    encode,\n",
    "    decode,\n",
    "    get_batch,\n",
    "    save_model_to_chekpoint,\n",
    "    estimate_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UErpTZ11FoJD"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rKPT8HZFp3f",
    "outputId": "a5f27423-d00e-4ffc-d8cc-1059a42708b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37443 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# raw data\n",
    "path_do_data = \"./ERA/Session17/datasets/GPT/data/english.txt\"\n",
    "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
    "# we use pretrained BERT tokenizer for performance improvements\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# data_raw = data_raw[4000000:] # short dataset\n",
    "\n",
    "# train/val split\n",
    "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKJjjaZlF3OL"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sao49iGkHIWt"
   },
   "outputs": [],
   "source": [
    "from super_repo.transformer import DecoderLayer\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # see more: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\", 100)\n",
    "        self.num_embed = kwargs.get(\"num_embed\", 32)\n",
    "        self.block_size = kwargs.get(\"block_size\", 8)\n",
    "        self.num_heads = kwargs.get(\"num_heads\", 4)\n",
    "        self.num_layers = kwargs.get(\"num_layers\", 4)\n",
    "        self.dropout = kwargs.get(\"dropout\", 0.2)\n",
    "        # each token reads the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "        # each position from 0 to block_size-1 will get its embedding\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, self.num_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                DecoderLayer(\n",
    "                    num_heads=self.num_heads,\n",
    "                    block_size=self.block_size,\n",
    "                    num_embed=self.num_embed,\n",
    "                    dropout=self.dropout,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # we add the layer norm before the Linear layer\n",
    "        self.ln_f = nn.LayerNorm(self.num_embed)\n",
    "        self.lm_head = nn.Linear(self.num_embed, self.vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are (B,T) tensor of integers\n",
    "        # the token_emb is (B, T, C), C = NUM_EMBED\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        # (T, C)\n",
    "        posit_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
    "\n",
    "        x = token_emb + posit_emb\n",
    "        # apply one head of self-attention\n",
    "        x = self.blocks(x)\n",
    "        # (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        # compute the loss\n",
    "        if targets != None:\n",
    "            # cross_entropy accepts inputs in a (batch_size, num_classes)\n",
    "            # so we need to reformat our logits dimensions to\n",
    "            # (batch_size * time, dim_vocabulary), time = block_size\n",
    "            B, T, C = logits.shape\n",
    "            logits = torch.reshape(logits, (B * T, C))\n",
    "            targets = torch.reshape(targets, (B * T,))\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, block_size: int):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context too the  last block_size tokens\n",
    "            # because tokens don't communicate between blocks\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crop)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution with probabilities probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJHHdNTFF4m_",
    "outputId": "4f9157e2-1b68-48e3-ad24-f18c40bc0a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 89.48M parameters\n"
     ]
    }
   ],
   "source": [
    "# train a new model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=NUM_EMBED,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEAD,\n",
    "    num_layers=NUM_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "# load model to GPU if available\n",
    "m = model.to(DEVICE)\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg6Ka2TXH2RD"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOwrSJ7jH3gA",
    "outputId": "f96d9607-34d3-4ca2-822e-b4e42cd34c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step          0 | train loss 10.7165 | val loss 10.7144\n",
      "step        499 | train loss 0.5163 | val loss 8.0840\n"
     ]
    }
   ],
   "source": [
    "# optimizer takes the model's parameters and the learning rate as input,\n",
    "# and updates the parameters during the training process in order to\n",
    "# minimize the loss function.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
    "MAX_ITER = 500\n",
    "for step in range(MAX_ITER):\n",
    "\n",
    "    # every EVAL_INTER evaluate the loss on train and val sets\n",
    "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
    "        loss_train = estimate_loss(data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "        loss_val = estimate_loss(data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backward() method on the loss variable calculates the gradients\n",
    "    # of the loss with respect to the model's parameters.\n",
    "    loss.backward()\n",
    "    # step() method on the optimizer updates the model's parameters\n",
    "    # using the calculated gradients, in order to minimize the loss.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW7J-q7FJtoB"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wdkrSVEKXJQ",
    "outputId": "a47e6658-15c5-48af-9902-56bba0c72464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]. as 3x3 is a boost of channels, we are not defeating directly. it in the animation. hence of the input, our network would request you to understand how fully connected layers. it had information, the network reason can be covered in the imagenet.. through an input image to that is where 1x1 convolutions, called implementation of transformers, we have been discussing alternative. ( also not ), an input image or patches can be helpful as shown in this image\n"
     ]
    }
   ],
   "source": [
    "# generate some output based on the context\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "print(\n",
    "    decode(\n",
    "        enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VJwm0f6Kbfm"
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fG4arVqLKdzu"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GPT.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
